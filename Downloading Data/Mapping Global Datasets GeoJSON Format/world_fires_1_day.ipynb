{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " latitude      0\n",
      "longitude     0\n",
      "brightness    0\n",
      "scan          0\n",
      "track         0\n",
      "acq_date      0\n",
      "acq_time      0\n",
      "satellite     0\n",
      "confidence    0\n",
      "version       0\n",
      "bright_t31    0\n",
      "frp           0\n",
      "daynight      0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Entries: 0\n",
      "\n",
      "Data Cleaning Complete. Cleaned dataset saved as 'cleaned_world_fires_1_day_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('eq_data/world_fires_1_day.csv')  # Replace 'your_dataset.csv' with the actual file path\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Check for duplicate entries\n",
    "duplicate_entries = data.duplicated().sum()\n",
    "print(\"\\nDuplicate Entries:\", duplicate_entries)\n",
    "\n",
    "# Data type check and conversion (if needed)\n",
    "data['acq_date'] = pd.to_datetime(data['acq_date'], format='%Y-%m-%d')  # Updated format to 'yyyy-mm-dd'\n",
    "\n",
    "# You can also convert other columns to appropriate data types if necessary\n",
    "\n",
    "# Drop duplicates (if you want to remove them)\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Fill missing values (if necessary)\n",
    "# Example: Fill missing values in 'brightness' column with the mean\n",
    "# data['brightness'].fillna(data['brightness'].mean(), inplace=True)\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "data.to_csv('cleaned_world_fires_1_day_dataset.csv', index=False)  # Replace 'cleaned_dataset.csv' with the desired file name\n",
    "\n",
    "print(\"\\nData Cleaning Complete. Cleaned dataset saved as 'cleaned_world_fires_1_day_dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "This report provides an overview of the data cleaning process conducted on the dataset. The dataset was examined for missing values and duplicate entries, and the appropriate data type conversions were applied.\n",
    "\n",
    "1. Missing Values:\n",
    "Upon careful examination of the dataset, it was determined that there are no missing values (NaN) in any of the columns. Each record in the dataset contains complete and non-null data. As a result, there is no requirement for further imputation or data filling processes.\n",
    "\n",
    "2. Duplicate Entries:\n",
    "The analysis also revealed that there are no duplicate rows within the dataset. Each row is unique, indicating the absence of any repeated or redundant observations. This ensures the integrity of the dataset, eliminating the need for deduplication procedures.\n",
    "\n",
    "### Conclusion:\n",
    "The dataset is devoid of missing values and duplicate entries, establishing a solid foundation for subsequent data analysis, visualization, and modeling phases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
